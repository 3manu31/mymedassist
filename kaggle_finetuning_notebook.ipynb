{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82836906",
   "metadata": {},
   "source": [
    "# Medical Study Assistant - LoRA Fine-tuning on Kaggle\n",
    "\n",
    "This notebook fine-tunes Qwen2.5-3B-Instruct using LoRA (Low-Rank Adaptation) on medical study assistant data.\n",
    "\n",
    "## Setup Requirements\n",
    "- **GPU**: P100 or better recommended\n",
    "- **Memory**: 16GB+ GPU memory\n",
    "- **Dataset**: Upload `medical_dataset_kaggle.jsonl` to Kaggle\n",
    "\n",
    "## Model Configuration\n",
    "- **Base**: Qwen/Qwen2.5-3B-Instruct\n",
    "- **Method**: LoRA fine-tuning\n",
    "- **Tasks**: Q&A + Study Guide Generation\n",
    "- **Domain**: Infectious Diseases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37490f8f",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abacb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.36.0\n",
    "!pip install trl==0.7.6\n",
    "!pip install peft==0.7.1\n",
    "!pip install datasets==2.16.0\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install bitsandbytes==0.41.3\n",
    "!pip install torch==2.1.0\n",
    "!pip install wandb  # Optional for logging\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3636a",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c829b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Transformers and training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set up logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"üî• CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üì± GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available - training will be very slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21759d56",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file and return list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Path to your uploaded dataset\n",
    "dataset_path = '/kaggle/input/medical-study-assistant/medical_dataset_kaggle.jsonl'\n",
    "\n",
    "# Load the data\n",
    "print(\"üìä Loading dataset...\")\n",
    "data = load_jsonl(dataset_path)\n",
    "print(f\"‚úÖ Loaded {len(data)} training examples\")\n",
    "\n",
    "# Display dataset statistics\n",
    "task_types = [entry['task_type'] for entry in data]\n",
    "topics = [entry['topic'] for entry in data]\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"  Q&A pairs: {task_types.count('question_answering')}\")\n",
    "print(f\"  Study guides: {task_types.count('study_guide_generation')}\")\n",
    "print(f\"  Unique topics: {len(set(topics))}\")\n",
    "print(f\"  Average text length: {np.mean([len(entry['output']) for entry in data]):.0f} chars\")\n",
    "\n",
    "# Show sample entries\n",
    "print(f\"\\nüîç Sample Entry:\")\n",
    "sample = data[0]\n",
    "print(f\"  Task: {sample['task_type']}\")\n",
    "print(f\"  Topic: {sample['topic']}\")\n",
    "print(f\"  Question: {sample['instruction'][:100]}...\")\n",
    "print(f\"  Answer: {sample['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d8748",
   "metadata": {},
   "source": [
    "## 4. Format Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for instruction following\n",
    "def format_instruction(entry):\n",
    "    \"\"\"Format a single training example for instruction following.\"\"\"\n",
    "    \n",
    "    # Create prompt template\n",
    "    if entry['input'].strip():\n",
    "        prompt = f\"\"\"Below is an instruction that describes a medical task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{entry['instruction']}\n",
    "\n",
    "### Input:\n",
    "{entry['input']}\n",
    "\n",
    "### Response:\n",
    "{entry['output']}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a medical task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{entry['instruction']}\n",
    "\n",
    "### Response:\n",
    "{entry['output']}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Format all entries\n",
    "print(\"üîÑ Formatting data for training...\")\n",
    "formatted_data = []\n",
    "for entry in data:\n",
    "    formatted_text = format_instruction(entry)\n",
    "    formatted_data.append({\n",
    "        'text': formatted_text,\n",
    "        'task_type': entry['task_type'],\n",
    "        'topic': entry['topic']\n",
    "    })\n",
    "\n",
    "# Create train/val split\n",
    "train_size = int(0.8 * len(formatted_data))\n",
    "train_data = formatted_data[:train_size]\n",
    "val_data = formatted_data[train_size:]\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(formatted_data)} examples\")\n",
    "print(f\"  Training: {len(train_data)} examples\")\n",
    "print(f\"  Validation: {len(val_data)} examples\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"\\nüìù Sample Formatted Training Example:\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07527c",
   "metadata": {},
   "source": [
    "## 5. Load Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a073284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "output_dir = \"./medical-study-assistant-lora\"\n",
    "\n",
    "# Quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"üî§ Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True\n",
    ")\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "print(f\"ü§ñ Loading base model from {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters() / 1e6:.1f}M\")\n",
    "print(f\"üíæ GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ecddf",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5261cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # rank\n",
    "    lora_alpha=32,  # alpha scaling parameter\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "print(\"üîß Applying LoRA configuration...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params, all_param\n",
    "\n",
    "trainable_params, all_param = print_trainable_parameters(model)\n",
    "print(f\"‚úÖ LoRA applied successfully!\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
    "print(f\"üìä Total parameters: {all_param / 1e6:.2f}M\")\n",
    "print(f\"üìä Trainable %: {100 * trainable_params / all_param:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683f0b2",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011acf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    group_by_length=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,  # Use fp16 for faster training\n",
    "    report_to=None,  # Disable wandb for now\n",
    "    # optim=\"adamw_torch\",\n",
    "    # seed=42,\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Total steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856586bd",
   "metadata": {},
   "source": [
    "## 8. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370cd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    packing=False,  # Don't pack sequences\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized successfully!\")\n",
    "print(f\"üìä Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"üìä Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"üíæ GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e328a",
   "metadata": {},
   "source": [
    "## 9. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79572c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"‚è∞ This may take 30-60 minutes depending on your GPU\")\n",
    "print(\"üìä Monitor the loss values to ensure training is progressing\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144131c",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a246d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"üíæ Saving trained model...\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save training metrics\n",
    "training_metrics = trainer.state.log_history\n",
    "with open(f\"{output_dir}/training_metrics.json\", 'w') as f:\n",
    "    json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "print(f\"üìä Training metrics saved to {output_dir}/training_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc52e0",
   "metadata": {},
   "source": [
    "## 11. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "def test_model(instruction, input_text=\"\"):\n",
    "    \"\"\"Test the model with a given instruction.\"\"\"\n",
    "    \n",
    "    # Format the prompt\n",
    "    if input_text.strip():\n",
    "        prompt = f\"\"\"Below is an instruction that describes a medical task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Below is an instruction that describes a medical task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the response part\n",
    "    response_start = response.find(\"### Response:\") + len(\"### Response:\")\n",
    "    response = response[response_start:].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What are the key clinical features of tuberculosis?\",\n",
    "    \"How is HIV diagnosed?\",\n",
    "    \"What are the treatment options for infective endocarditis?\",\n",
    "    \"Describe the pathophysiology of brain abscess.\",\n",
    "    \"Create a study guide for fungal infections.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing the fine-tuned model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nüîç Test {i}: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        answer = test_model(question)\n",
    "        print(f\"üí° Answer: {answer[:300]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdfd95e",
   "metadata": {},
   "source": [
    "## 12. Export Model for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f380f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tarball of the model for download\n",
    "import shutil\n",
    "\n",
    "print(\"üì¶ Creating model archive for download...\")\n",
    "\n",
    "# Create archive\n",
    "archive_name = \"medical-study-assistant-lora\"\n",
    "shutil.make_archive(archive_name, 'zip', output_dir)\n",
    "\n",
    "# Check file size\n",
    "archive_path = f\"{archive_name}.zip\"\n",
    "archive_size = os.path.getsize(archive_path) / 1024**2  # MB\n",
    "\n",
    "print(f\"‚úÖ Model archive created: {archive_path}\")\n",
    "print(f\"üìä Archive size: {archive_size:.1f} MB\")\n",
    "print(f\"üì• Download this file to use the model locally\")\n",
    "\n",
    "# Create a simple README for the model\n",
    "readme_content = f\"\"\"# Medical Study Assistant - LoRA Fine-tuned Model\n",
    "\n",
    "This model is a fine-tuned version of Qwen2.5-3B-Instruct specialized for medical study assistance.\n",
    "\n",
    "## Model Details\n",
    "- **Base Model**: Qwen/Qwen2.5-3B-Instruct\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\n",
    "- **Training Data**: {len(train_dataset)} medical Q&A and study guide examples\n",
    "- **Domain**: Infectious Diseases\n",
    "- **Tasks**: Question Answering, Study Guide Generation\n",
    "\n",
    "## Usage\n",
    "1. Extract the model files\n",
    "2. Load using HuggingFace Transformers and PEFT\n",
    "3. Use the instruction format for best results\n",
    "\n",
    "## Training Configuration\n",
    "- LoRA Rank: 16\n",
    "- LoRA Alpha: 32\n",
    "- Learning Rate: 5e-4\n",
    "- Training Epochs: 3\n",
    "- Batch Size: 2 (effective: 16 with gradient accumulation)\n",
    "\n",
    "## Performance\n",
    "- Specializes in infectious disease topics\n",
    "- Generates exam-focused study materials\n",
    "- Provides detailed medical explanations\n",
    "\n",
    "---\n",
    "Generated using Kaggle GPU on {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{output_dir}/README.md\", 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"üìÑ README.md created with model information\")\n",
    "print(f\"üéâ Fine-tuning completed successfully!\")\n",
    "print(f\"\\nüìã Next Steps:\")\n",
    "print(f\"1. Download the model archive ({archive_path})\")\n",
    "print(f\"2. Extract and integrate into your local environment\")\n",
    "print(f\"3. Test with your specific medical questions\")\n",
    "print(f\"4. Consider further fine-tuning on additional data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
